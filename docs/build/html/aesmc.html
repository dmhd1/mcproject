

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>aesmc package &mdash; Recreate Aesmc  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="models package" href="models.html" />
    <link rel="prev" title="Welcome to aesmc_recreate’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> Recreate Aesmc
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">aesmc package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-aesmc.inference">aesmc.inference module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-aesmc.losses">aesmc.losses module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-aesmc.math">aesmc.math module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-aesmc.state">aesmc.state module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-aesmc.statistics">aesmc.statistics module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-aesmc.train">aesmc.train module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-aesmc">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="models.html">models package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Recreate Aesmc</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>aesmc package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/aesmc.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="aesmc-package">
<h1>aesmc package<a class="headerlink" href="#aesmc-package" title="Permalink to this headline">¶</a></h1>
<p>Code for Auto-Encoding Sequential Monte Carlo:
[1] Le, T. A., Igl, M., Rainforth, T., Jin, T., &amp; Wood, F. (2017). Auto-encoding sequential monte carlo. arXiv preprint
arXiv:1705.10306.</p>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-aesmc.inference">
<span id="aesmc-inference-module"></span><h2>aesmc.inference module<a class="headerlink" href="#module-aesmc.inference" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="aesmc.inference.get_resampled_latents">
<code class="sig-prename descclassname">aesmc.inference.</code><code class="sig-name descname">get_resampled_latents</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">latents</span></em>, <em class="sig-param"><span class="n">ancestral_indices</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.inference.get_resampled_latents" title="Permalink to this definition">¶</a></dt>
<dd><p>Resample list of latents.</p>
<dl>
<dt>Args:</dt><dd><p>latents: list of tensors [batch_size, num_particles] or dicts thereof
ancestral_indices: list where each element is a LongTensor</p>
<blockquote>
<div><p>[batch_size, num_particles] of length (len(latents) - 1); can
be empty.</p>
</div></blockquote>
</dd>
</dl>
<p>Returns: list of elements of the same type as latents</p>
</dd></dl>

<dl class="py function">
<dt id="aesmc.inference.infer">
<code class="sig-prename descclassname">aesmc.inference.</code><code class="sig-name descname">infer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inference_algorithm</span></em>, <em class="sig-param"><span class="n">observations</span></em>, <em class="sig-param"><span class="n">initial</span></em>, <em class="sig-param"><span class="n">transition</span></em>, <em class="sig-param"><span class="n">emission</span></em>, <em class="sig-param"><span class="n">proposal</span></em>, <em class="sig-param"><span class="n">num_particles</span></em>, <em class="sig-param"><span class="n">return_log_marginal_likelihood</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_latents</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">return_original_latents</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_log_weight</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">return_log_weights</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">return_ancestral_indices</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.inference.infer" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform inference on a state space model using either sequential Monte
Carlo or importance sampling.</p>
<dl>
<dt>Args:</dt><dd><p>inference_algorithm: is or smc (string)
observations: list of tensors [batch_size, …] or</p>
<blockquote>
<div><p>dicts thereof of length num_timesteps</p>
</div></blockquote>
<dl>
<dt>initial: a callable object (function or nn.Module) which has no</dt><dd><p>arguments and returns a torch.distributions.Distribution or a dict
thereof</p>
</dd>
<dt>transition: a callable object (function or nn.Module) with signature:</dt><dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>previous_latents: list of length time where each element is a</dt><dd><p>tensor [batch_size, num_particles, …]</p>
</dd>
</dl>
<p>time: int (zero-indexed)
previous_observations: list of length time where each element</p>
<blockquote>
<div><p>is a tensor [batch_size, …] or a dict thereof</p>
</div></blockquote>
</dd>
</dl>
<p>Returns: torch.distributions.Distribution or a dict thereof</p>
</dd>
<dt>emission: a callable object (function or nn.Module) with signature:</dt><dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>latents: list of length (time + 1) where each element is a</dt><dd><p>tensor [batch_size, num_particles, …]</p>
</dd>
</dl>
<p>time: int (zero-indexed)
previous_observations: list of length time where each element</p>
<blockquote>
<div><p>is a tensor [batch_size, …] or a dict thereof</p>
</div></blockquote>
</dd>
</dl>
<p>Returns: torch.distributions.Distribution or a dict thereof</p>
</dd>
<dt>proposal: a callable object (function or nn.Module) with signature:</dt><dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>previous_latents: list of length time where each element is a</dt><dd><p>tensor [batch_size, num_particles, …]</p>
</dd>
</dl>
<p>time: int (zero-indexed)
observations: list of length num_timesteps where each element</p>
<blockquote>
<div><p>is a tensor [batch_size, …] or a dict thereof</p>
</div></blockquote>
</dd>
</dl>
<p>Returns: torch.distributions.Distribution or a dict thereof</p>
</dd>
</dl>
<p>num_particles: int; number of particles
return_log_marginal_likelihood: bool (default: False)
return_latents: bool (default: True)
return_original_latents: bool (default: False); only applicable for smc
return_log_weight: bool (default: True)
return_log_weights: bool (default: False)
return_ancestral_indices: bool (default: False); only applicable for</p>
<blockquote>
<div><p>smc</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>a dict containing key-value pairs for a subset of the following keys
as specified by the return_{} parameters:</p>
<blockquote>
<div><p>log_marginal_likelihood: tensor [batch_size]
latents: list of tensors (or dict thereof)</p>
<blockquote>
<div><p>[batch_size, num_particles, …] of length len(observations)</p>
</div></blockquote>
<dl class="simple">
<dt>original_latents: list of tensors (or dict thereof)</dt><dd><p>[batch_size, num_particles, …] of length len(observations)</p>
</dd>
</dl>
<p>log_weight: tensor [batch_size, num_particles]
log_weights: list of tensors [batch_size, num_particles]</p>
<blockquote>
<div><p>of length len(observations)</p>
</div></blockquote>
<dl class="simple">
<dt>ancestral_indices: list of <a href="#id1"><span class="problematic" id="id2">`</span></a>torch.LongTensor`s</dt><dd><p>[batch_size, num_particles] of length len(observations)</p>
</dd>
</dl>
</div></blockquote>
<p>Note that (latents, log_weight) characterize the posterior.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="aesmc.inference.sample_ancestral_index">
<code class="sig-prename descclassname">aesmc.inference.</code><code class="sig-name descname">sample_ancestral_index</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_weight</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.inference.sample_ancestral_index" title="Permalink to this definition">¶</a></dt>
<dd><p>Sample ancestral index using systematic resampling.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>log_weight: log of unnormalized weights, tensor</dt><dd><p>[batch_size, num_particles]</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>zero-indexed ancestral index: LongTensor [batch_size, num_particles]</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-aesmc.losses">
<span id="aesmc-losses-module"></span><h2>aesmc.losses module<a class="headerlink" href="#module-aesmc.losses" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="aesmc.losses.get_loss">
<code class="sig-prename descclassname">aesmc.losses.</code><code class="sig-name descname">get_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">observations</span></em>, <em class="sig-param"><span class="n">num_particles</span></em>, <em class="sig-param"><span class="n">algorithm</span></em>, <em class="sig-param"><span class="n">initial</span></em>, <em class="sig-param"><span class="n">transition</span></em>, <em class="sig-param"><span class="n">emission</span></em>, <em class="sig-param"><span class="n">proposal</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.losses.get_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a differentiable loss for gradient descent.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>observations: list of tensors [batch_size, dim1, …, dimN] or</dt><dd><p>dicts thereof</p>
</dd>
</dl>
<p>num_particles: int
algorithm: iwae or aesmc
initial: a callable object (function or nn.Module) which has no</p>
<blockquote>
<div><p>arguments and returns a torch.distributions.Distribution or a dict
thereof</p>
</div></blockquote>
<dl>
<dt>transition: a callable object (function or nn.Module) with signature:</dt><dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>previous_latents: list of length time where each element is a</dt><dd><p>tensor [batch_size, num_particles, …]</p>
</dd>
</dl>
<p>time: int (zero-indexed)
previous_observations: list of length time where each element</p>
<blockquote>
<div><p>is a tensor [batch_size, …] or a dict thereof</p>
</div></blockquote>
</dd>
</dl>
<p>Returns: torch.distributions.Distribution or a dict thereof</p>
</dd>
<dt>emission: a callable object (function or nn.Module) with signature:</dt><dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>latents: list of length (time + 1) where each element is a</dt><dd><p>tensor [batch_size, num_particles, …]</p>
</dd>
</dl>
<p>time: int (zero-indexed)
previous_observations: list of length time where each element</p>
<blockquote>
<div><p>is a tensor [batch_size, …] or a dict thereof</p>
</div></blockquote>
</dd>
</dl>
<p>Returns: torch.distributions.Distribution or a dict thereof</p>
</dd>
<dt>proposal: a callable object (function or nn.Module) with signature:</dt><dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>previous_latents: list of length time where each element is a</dt><dd><p>tensor [batch_size, num_particles, …]</p>
</dd>
</dl>
<p>time: int (zero-indexed)
observations: list of length num_timesteps where each element</p>
<blockquote>
<div><p>is a tensor [batch_size, …] or a dict thereof</p>
</div></blockquote>
</dd>
</dl>
<p>Returns: torch.distributions.Distribution or a dict thereof</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>loss: scalar that we call .backward() on and step the optimizer.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-aesmc.math">
<span id="aesmc-math-module"></span><h2>aesmc.math module<a class="headerlink" href="#module-aesmc.math" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="aesmc.math.exponentiate_and_normalize">
<code class="sig-prename descclassname">aesmc.math.</code><code class="sig-name descname">exponentiate_and_normalize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">dim</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.math.exponentiate_and_normalize" title="Permalink to this definition">¶</a></dt>
<dd><p>Exponentiates and normalizes a torch.Tensor/np.ndarray.</p>
<dl>
<dt>Args:</dt><dd><p>values: torch.Tensor/np.ndarray [dim_1, …, dim_N]
dim: n</p>
</dd>
<dt>Returns:</dt><dd><dl>
<dt>result: torch.Tensor/np.ndarray [dim_1, …, dim_N]</dt><dd><p>where result[i_1, …, i_N] =</p>
<blockquote>
<div><p>exp(values[i_1, …, i_N])</p>
</div></blockquote>
<blockquote>
<div><p>sum_{j = 1}^{dim_n} exp(values[i_1, …, j, …, i_N])</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="aesmc.math.lognormexp">
<code class="sig-prename descclassname">aesmc.math.</code><code class="sig-name descname">lognormexp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">values</span></em>, <em class="sig-param"><span class="n">dim</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.math.lognormexp" title="Permalink to this definition">¶</a></dt>
<dd><p>Exponentiates, normalizes and takes log of a torch.Tensor/np.ndarray.</p>
<dl>
<dt>Args:</dt><dd><p>values: torch.Tensor/np.ndarray [dim_1, …, dim_N]
dim: n</p>
</dd>
<dt>Returns:</dt><dd><dl>
<dt>result: torch.Tensor/np.ndarray [dim_1, …, dim_N]</dt><dd><p>where result[i_1, …, i_N] =</p>
<blockquote>
<div><p>exp(values[i_1, …, i_N])</p>
</div></blockquote>
<dl class="simple">
<dt>log( ———————————————————— )</dt><dd><p>sum_{j = 1}^{dim_n} exp(values[i_1, …, j, …, i_N])</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-aesmc.state">
<span id="aesmc-state-module"></span><h2>aesmc.state module<a class="headerlink" href="#module-aesmc.state" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="aesmc.state.BatchShapeMode">
<em class="property">class </em><code class="sig-prename descclassname">aesmc.state.</code><code class="sig-name descname">BatchShapeMode</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.state.BatchShapeMode" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="py attribute">
<dt id="aesmc.state.BatchShapeMode.BATCH_EXPANDED">
<code class="sig-name descname">BATCH_EXPANDED</code><em class="property"> = 1</em><a class="headerlink" href="#aesmc.state.BatchShapeMode.BATCH_EXPANDED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="aesmc.state.BatchShapeMode.FULLY_EXPANDED">
<code class="sig-name descname">FULLY_EXPANDED</code><em class="property"> = 2</em><a class="headerlink" href="#aesmc.state.BatchShapeMode.FULLY_EXPANDED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="aesmc.state.BatchShapeMode.NOT_EXPANDED">
<code class="sig-name descname">NOT_EXPANDED</code><em class="property"> = 0</em><a class="headerlink" href="#aesmc.state.BatchShapeMode.NOT_EXPANDED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="aesmc.state.expand_observation">
<code class="sig-prename descclassname">aesmc.state.</code><code class="sig-name descname">expand_observation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">observation</span></em>, <em class="sig-param"><span class="n">num_particles</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.state.expand_observation" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>observation: <cite>torch.Tensor</cite> [batch_size, …] or <cite>dict</cite></dt><dd><p>thereof</p>
</dd>
</dl>
<p>num_particles: int</p>
</dd>
<dt>Returns: <cite>torch.Tensor</cite> [batch_size, num_particles, …] or</dt><dd><p><cite>dict</cite> thereof</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="aesmc.state.get_batch_shape_mode">
<code class="sig-prename descclassname">aesmc.state.</code><code class="sig-name descname">get_batch_shape_mode</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">distribution</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_particles</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.state.get_batch_shape_mode" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the BatchShapeMode property of a distribution.
If the property is not set explicitly, it is inferred implicitly.</p>
</dd></dl>

<dl class="py function">
<dt id="aesmc.state.log_prob">
<code class="sig-prename descclassname">aesmc.state.</code><code class="sig-name descname">log_prob</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">distribution</span></em>, <em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.state.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>Log probability of value under distribution.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>distribution: <cite>torch.distributions.Distribution</cite> of batch_shape either</dt><dd><p>[batch_size, num_particles, …] or
[batch_size, …] or
[…] or <cite>dict</cite> thereof.</p>
</dd>
<dt>value: <cite>torch.Tensor</cite> of size</dt><dd><p>[batch_size, num_particles, …] + distribution.event_shape
or <cite>dict</cite> thereof</p>
</dd>
</dl>
</dd>
</dl>
<p>Returns: <cite>torch.Tensor</cite> [batch_size, num_particles] or <cite>dict</cite> thereof</p>
</dd></dl>

<dl class="py function">
<dt id="aesmc.state.resample">
<code class="sig-prename descclassname">aesmc.state.</code><code class="sig-name descname">resample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">ancestral_index</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.state.resample" title="Permalink to this definition">¶</a></dt>
<dd><p>Resample the value without side effects.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>value: <cite>torch.Tensor</cite> [batch_size, num_particles, dim_1, …, dim_N]</dt><dd><p>(or [batch_size, num_particles]) or <cite>dict</cite> thereof</p>
</dd>
</dl>
<p>ancestral_index: <cite>torch.LongTensor</cite> [batch_size, num_particles]</p>
</dd>
<dt>Returns: resampled value [batch_size, num_particles, dim_1, …, dim_N]</dt><dd><p>(or [batch_size, num_particles]) or <cite>dict</cite> thereof</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="aesmc.state.sample">
<code class="sig-prename descclassname">aesmc.state.</code><code class="sig-name descname">sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">distribution</span></em>, <em class="sig-param"><span class="n">batch_size</span></em>, <em class="sig-param"><span class="n">num_particles</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.state.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples from a distribution given batch size and number of particles.</p>
<dl>
<dt>Args:</dt><dd><p>distribution: <cite>torch.distributions.Distribution</cite> or <cite>dict</cite> thereof.</p>
<blockquote>
<div><p>Note: the batch_shape of distribution can have one of the following
batch shape modes: […],
[batch_size, …],
[batch_size, num_particles, …]. The batch shape mode
of a distribution can be set explicitly using the
<cite>set_batch_shape_mode</cite> function. If not set, the batch shape mode
is inferred, although there can be ambiguities.</p>
</div></blockquote>
<p>batch_size: <cite>int</cite>
num_particles: <cite>int</cite></p>
</dd>
</dl>
<p>Returns: <cite>torch.Tensor</cite> [batch_size, num_particles, …] or <cite>dict</cite> thereof</p>
</dd></dl>

<dl class="py function">
<dt id="aesmc.state.set_batch_shape_mode">
<code class="sig-prename descclassname">aesmc.state.</code><code class="sig-name descname">set_batch_shape_mode</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">distribution</span></em>, <em class="sig-param"><span class="n">batch_shape_mode</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.state.set_batch_shape_mode" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the BatchShapeMode property of a distribution
explicitly.</p>
</dd></dl>

</div>
<div class="section" id="module-aesmc.statistics">
<span id="aesmc-statistics-module"></span><h2>aesmc.statistics module<a class="headerlink" href="#module-aesmc.statistics" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="aesmc.statistics.empirical_expectation">
<code class="sig-prename descclassname">aesmc.statistics.</code><code class="sig-name descname">empirical_expectation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">log_weight</span></em>, <em class="sig-param"><span class="n">f</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.statistics.empirical_expectation" title="Permalink to this definition">¶</a></dt>
<dd><p>Empirical expectation.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>value: torch.Tensor</dt><dd><p>[batch_size, num_particles, value_dim_1, …, value_dim_N] (or
[batch_size, num_particles])</p>
</dd>
</dl>
<p>log_weight: torch.Tensor [batch_size, num_particles]
f: function which takes torch.Tensor</p>
<blockquote>
<div><p>[batch_size, value_dim_1, …, value_dim_N] (or
[batch_size]) and returns a torch.Tensor
[batch_size, dim_1, …, dim_M] (or [batch_size])</p>
</div></blockquote>
</dd>
<dt>Returns: empirical expectation torch.Tensor</dt><dd><p>[batch_size, dim_1, …, dim_M] (or [batch_size])</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="aesmc.statistics.empirical_mean">
<code class="sig-prename descclassname">aesmc.statistics.</code><code class="sig-name descname">empirical_mean</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">log_weight</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.statistics.empirical_mean" title="Permalink to this definition">¶</a></dt>
<dd><p>Empirical mean.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>value: torch.Tensor</dt><dd><p>[batch_size, num_particles, dim_1, …, dim_N] (or
[batch_size, num_particles])</p>
</dd>
</dl>
<p>log_weight: torch.Tensor [batch_size, num_particles]</p>
</dd>
<dt>Returns: empirical mean torch.Tensor</dt><dd><p>[batch_size, dim_1, …, dim_N] (or [batch_size])</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="aesmc.statistics.empirical_variance">
<code class="sig-prename descclassname">aesmc.statistics.</code><code class="sig-name descname">empirical_variance</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em>, <em class="sig-param"><span class="n">log_weight</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.statistics.empirical_variance" title="Permalink to this definition">¶</a></dt>
<dd><p>Empirical variance.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>value: torch.Tensor</dt><dd><p>[batch_size, num_particles, dim_1, …, dim_N] (or
[batch_size, num_particles])</p>
</dd>
</dl>
<p>log_weight: torch.Tensor [batch_size, num_particles]</p>
</dd>
<dt>Returns: empirical mean torch.Tensor</dt><dd><p>[batch_size, dim_1, …, dim_N] (or [batch_size])</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="aesmc.statistics.ess">
<code class="sig-prename descclassname">aesmc.statistics.</code><code class="sig-name descname">ess</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_weight</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.statistics.ess" title="Permalink to this definition">¶</a></dt>
<dd><p>Effective sample size.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>log_weight: Unnormalized log weights</dt><dd><p>torch.Tensor [batch_size, num_particles] (or [num_particles])</p>
</dd>
</dl>
</dd>
</dl>
<p>Returns: effective sample size [batch_size] (or [1])</p>
</dd></dl>

<dl class="py function">
<dt id="aesmc.statistics.log_ess">
<code class="sig-prename descclassname">aesmc.statistics.</code><code class="sig-name descname">log_ess</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_weight</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.statistics.log_ess" title="Permalink to this definition">¶</a></dt>
<dd><p>Log of Effective sample size.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>log_weight: Unnormalized log weights</dt><dd><p>torch.Tensor [batch_size, num_particles] (or [num_particles])</p>
</dd>
</dl>
</dd>
</dl>
<p>Returns: log of effective sample size [batch_size] (or [1])</p>
</dd></dl>

<dl class="py function">
<dt id="aesmc.statistics.sample_from_prior">
<code class="sig-prename descclassname">aesmc.statistics.</code><code class="sig-name descname">sample_from_prior</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">initial</span></em>, <em class="sig-param"><span class="n">transition</span></em>, <em class="sig-param"><span class="n">emission</span></em>, <em class="sig-param"><span class="n">num_timesteps</span></em>, <em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.statistics.sample_from_prior" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples latents and observations from prior</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>initial: a callable object (function or nn.Module) which has no</dt><dd><p>arguments and returns a torch.distributions.Distribution or a dict
thereof</p>
</dd>
<dt>transition: a callable object (function or nn.Module) with signature:</dt><dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>previous_latents: list of length time where each element is a</dt><dd><p>tensor [batch_size, num_particles, …]</p>
</dd>
</dl>
<p>time: int (zero-indexed)
previous_observations: list of length time where each element</p>
<blockquote>
<div><p>is a tensor [batch_size, …] or a dict thereof</p>
</div></blockquote>
</dd>
</dl>
<p>Returns: torch.distributions.Distribution or a dict thereof</p>
</dd>
<dt>emission: a callable object (function or nn.Module) with signature:</dt><dd><dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>latents: list of length (time + 1) where each element is a</dt><dd><p>tensor [batch_size, num_particles, …]</p>
</dd>
</dl>
<p>time: int (zero-indexed)
previous_observations: list of length time where each element</p>
<blockquote>
<div><p>is a tensor [batch_size, …] or a dict thereof</p>
</div></blockquote>
</dd>
</dl>
<p>Returns: torch.distributions.Distribution or a dict thereof</p>
</dd>
</dl>
<p>num_timesteps: int
batch_size: int</p>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>latents: list of tensors (or dict thereof) [batch_size] of length</dt><dd><p>len(observations)</p>
</dd>
</dl>
<p>observations: list of tensors [batch_size, …] or dicts thereof</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-aesmc.train">
<span id="aesmc-train-module"></span><h2>aesmc.train module<a class="headerlink" href="#module-aesmc.train" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="aesmc.train.SyntheticDataset">
<em class="property">class </em><code class="sig-prename descclassname">aesmc.train.</code><code class="sig-name descname">SyntheticDataset</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">initial</span></em>, <em class="sig-param"><span class="n">transition</span></em>, <em class="sig-param"><span class="n">emission</span></em>, <em class="sig-param"><span class="n">num_timesteps</span></em>, <em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.train.SyntheticDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.dataset.Dataset</span></code></p>
</dd></dl>

<dl class="py function">
<dt id="aesmc.train.get_chained_params">
<code class="sig-prename descclassname">aesmc.train.</code><code class="sig-name descname">get_chained_params</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">objects</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.train.get_chained_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="aesmc.train.get_synthetic_dataloader">
<code class="sig-prename descclassname">aesmc.train.</code><code class="sig-name descname">get_synthetic_dataloader</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">initial</span></em>, <em class="sig-param"><span class="n">transition</span></em>, <em class="sig-param"><span class="n">emission</span></em>, <em class="sig-param"><span class="n">num_timesteps</span></em>, <em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.train.get_synthetic_dataloader" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="aesmc.train.train">
<code class="sig-prename descclassname">aesmc.train.</code><code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">dataloader</em>, <em class="sig-param">num_particles</em>, <em class="sig-param">algorithm</em>, <em class="sig-param">initial</em>, <em class="sig-param">transition</em>, <em class="sig-param">emission</em>, <em class="sig-param">proposal</em>, <em class="sig-param">num_epochs</em>, <em class="sig-param">num_iterations_per_epoch=None</em>, <em class="sig-param">optimizer_algorithm=&lt;class 'torch.optim.adam.Adam'&gt;</em>, <em class="sig-param">optimizer_kwargs={}</em>, <em class="sig-param">callback=None</em><span class="sig-paren">)</span><a class="headerlink" href="#aesmc.train.train" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-aesmc">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-aesmc" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="models.html" class="btn btn-neutral float-right" title="models package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to aesmc_recreate’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>